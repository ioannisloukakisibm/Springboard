{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "stock-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Dpac\n",
    "url = 'https://app.ticketmaster.com/discovery/v2/events.json?venueId=KovZpa2X8e&countryCode=US&apikey=Lqseo1SMwgdKqLEKOPTLlfJaLSCMtd0S'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "data=response.text\n",
    "\n",
    "parsed_dpac = json.loads(data)\n",
    "\n",
    "\n",
    "# Carolina Theatre\n",
    "url = 'https://app.ticketmaster.com/discovery/v2/events.json?venueId=KovZpZAFAl6A&countryCode=US&apikey=Lqseo1SMwgdKqLEKOPTLlfJaLSCMtd0S'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response)\n",
    "data=response.text\n",
    "\n",
    "parsed_carolina_theater = json.loads(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-raising",
   "metadata": {},
   "source": [
    "# GET EVENT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "stone-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the events\n",
    "list_of_events = parsed_dpac.get('_embedded').get('events')\n",
    "list_of_events.extend(parsed_carolina_theater.get('_embedded').get('events'))\n",
    "\n",
    "# Put all the events in a dataframe\n",
    "events = pd.DataFrame({'events':list_of_events})\n",
    "\n",
    "# Break the disctionaries into columns\n",
    "# Now we have 1 column for each sales, locale, dates, etc\n",
    "iteration_1 = events['events'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "distinguished-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "relevant-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the event names and add them to a dataframe\n",
    "list_of_event_names = [event.get('name') for event in list_of_events]\n",
    "list_of_event_ids = [event.get('id') for event in list_of_events]\n",
    "event_names_df = pd.DataFrame({'event name':list_of_event_names, 'event id':list_of_event_ids})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-essence",
   "metadata": {},
   "source": [
    "## SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "patent-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the sales column into columns\n",
    "# We now have public sales and presales\n",
    "sales = iteration_1.sales.apply(pd.Series)\n",
    "\n",
    "# Break the public sales into columns\n",
    "sales_public = sales.public.apply(pd.Series)\n",
    "\n",
    "# rename the public sales columns to distinguish them from the rest\n",
    "for col in sales_public.columns:\n",
    "    sales_public.rename({col:f'public sales {col}'}, axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "\n",
    "# Create a dataframe to add the final product of the steps below\n",
    "presale_final = pd.DataFrame()\n",
    "\n",
    "# Looping through the presales column to extract the information from the dictionaries\n",
    "# each presales row will have a dictionary with all the presale methods \n",
    "for presales_list_element in sales.presales:\n",
    "    \n",
    "# \"\"\"\n",
    "# If there is no presale, the entire rowcell will be NaN\n",
    "# In this case the pandas.isnull method will give us an error\n",
    "# We will need to add a new blank row in the except statement\n",
    "# We will also add a new column called 'blank' to make sure we don't mess the other columns names\n",
    "# The column will be dropped at the end\n",
    "# \"\"\"\n",
    "    try:\n",
    "        length = len(pd.isnull(presales_list_element))\n",
    "    except:\n",
    "        dict_holder_df = pd.DataFrame({'blank':[np.nan]})\n",
    "        presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "        continue\n",
    "\n",
    "# \"\"\"\n",
    "# The dictionaries will contain the name of the presale as well as the end and start date. \n",
    "# We need to extract the name and move it to the column names so that we can distinguish the start and end dates of each presale\n",
    "# \"\"\"\n",
    "    dict_holder_df = pd.DataFrame()\n",
    "    for presale_dict in presales_list_element:\n",
    "        \n",
    "        start_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[0]\n",
    "        end_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[1]\n",
    "        \n",
    "        holder_df = pd.DataFrame({start_time_title:[presale_dict['startDateTime']], end_time_title:[presale_dict['endDateTime']]})\n",
    "        dict_holder_df = pd.concat([dict_holder_df,holder_df], axis=1)\n",
    "        \n",
    "    presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "\n",
    "\n",
    "# Reset the index so that we can merge\n",
    "# drop the blank column\n",
    "presale_final.reset_index(inplace=True)\n",
    "presale_final.drop(columns=['index', 'blank'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-grave",
   "metadata": {},
   "source": [
    "# DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "touched-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the dates column into columns\n",
    "# There are a lot of dates trapped into disctionaries. Use pd.Series multiple times to untangle everything\n",
    "dates = iteration_1.dates.apply(pd.Series)\n",
    "start_dates = dates.start.apply(pd.Series)\n",
    "status = dates.status.apply(pd.Series)\n",
    "initial_start_date = dates.initialStartDate.apply(pd.Series)\n",
    "\n",
    "# Make sure there are no weird columns created by pd.series\n",
    "initial_start_date = initial_start_date[['dateTime', 'localDate', 'localTime']]\n",
    "\n",
    "# rename the dates columns to distinguish them\n",
    "for col in start_dates.columns:\n",
    "    start_dates.rename({col:f'event start {col}'}, axis = 1, inplace = True)\n",
    "\n",
    "for col in initial_start_date.columns:\n",
    "    initial_start_date.rename({col:f'event initial start {col}'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-proof",
   "metadata": {},
   "source": [
    "# CLASSIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "reliable-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "list_of_classification_dictionaries = [iteration_1.classifications[index][0] for index in range(len(iteration_1.classifications))] \n",
    "\n",
    "classification_df = pd.DataFrame({'classification_dictionaries':list_of_classification_dictionaries})\n",
    "classification_df_broken_in_columns = classification_df['classification_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "classification_df_primary_family = classification_df_broken_in_columns[['primary', 'family']]\n",
    "columns_of_interest = list(classification_df_broken_in_columns.columns)\n",
    "columns_of_interest.remove('primary')\n",
    "columns_of_interest.remove('family')\n",
    "\n",
    "classification_df_final = pd.DataFrame()\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    series_holder = classification_df_broken_in_columns[column].apply(pd.Series)\n",
    "    series_holder.drop(columns = ['id'], inplace=True)\n",
    "    series_holder.rename({series_holder.columns[0]:column}, inplace = True, axis = 1)\n",
    "    classification_df_final = pd.concat([classification_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-spring",
   "metadata": {},
   "source": [
    "# PROMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "expected-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the promoters column into columns\n",
    "promoters = iteration_1.promoters.apply(pd.Series)\n",
    "\n",
    "for index in range(len(promoters.columns)):\n",
    "    promoters.rename({promoters.columns[index]: 'event promoter' + ' ' + str(promoters.columns[index]+1)}, axis = 1, inplace = True)\n",
    "\n",
    "promoters_df_final = pd.DataFrame()\n",
    "\n",
    "for column in promoters.columns:\n",
    "    series_holder = promoters[column].apply(pd.Series)\n",
    "    series_holder = series_holder[['name', 'description']]\n",
    "    series_holder.rename({'name':column + ' ' + 'name', 'description':column + ' ' + 'description'}, inplace = True, axis = 1)\n",
    "    promoters_df_final = pd.concat([promoters_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-guitar",
   "metadata": {},
   "source": [
    "# PRICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ancient-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pricerange_dictionaries = [iteration_1.priceRanges[index][0] for index in range(len(iteration_1.priceRanges))] \n",
    "\n",
    "pricerange_df = pd.DataFrame({'pricerange_dictionaries':list_of_pricerange_dictionaries})\n",
    "pricerange_df_broken_in_columns = pricerange_df['pricerange_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "pricerange_df_broken_in_columns = pricerange_df_broken_in_columns[['type', 'min', 'max']]\n",
    "pricerange_df_broken_in_columns.rename({'type':'price type', 'min':'price min', 'max':'price max'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-leeds",
   "metadata": {},
   "source": [
    "# TICKET LIMIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "alleged-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_1.columns\n",
    "ticket_limit = pd.DataFrame(iteration_1['ticketLimit'].apply(pd.Series))\n",
    "ticket_limit.rename({'info':'ticket limit'},axis = 1, inplace = True)\n",
    "\n",
    "please_note = pd.DataFrame(iteration_1['pleaseNote'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-assumption",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "atlantic-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pull resulted in 0 new events\n"
     ]
    }
   ],
   "source": [
    "new_dataset = pd.concat([\n",
    "    event_names_df\n",
    "    ,iteration_1['info']\n",
    "    ,sales_public\n",
    "    ,presale_final\n",
    "    ,start_dates\n",
    "    ,status\n",
    "    ,initial_start_date\n",
    "    ,classification_df_final\n",
    "    ,promoters_df_final\n",
    "    ,pricerange_df_broken_in_columns\n",
    "    ,ticket_limit\n",
    "    ,please_note\n",
    "    \n",
    "], axis = 1)\n",
    "\n",
    "\n",
    "try:\n",
    "    current_dataset = pd.read_csv('final_ticketmaster_dataset.csv')\n",
    "    current_ids = list(current_dataset['event id'])\n",
    "\n",
    "    new_ids = list(new_dataset['event id'])\n",
    "\n",
    "    updated_dataset = pd.concat([current_dataset, new_dataset[new_dataset['event id'].isin(np.setdiff1d(new_ids,current_ids))]], axis = 0)\n",
    "\n",
    "    updated_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "\n",
    "    extra_ids = list(np.setdiff1d(new_ids,current_ids))\n",
    "    print ('This pull resulted in', len(extra_ids), 'new events')\n",
    "\n",
    "\n",
    "except:\n",
    "    new_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "    print('except run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-course",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
