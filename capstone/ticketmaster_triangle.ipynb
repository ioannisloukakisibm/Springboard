{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "api_key = input('Enter your API key')\n",
    "\n",
    "index = 1\n",
    "list_of_events = []\n",
    "\n",
    "while True:\n",
    "\n",
    "    url = f'https://app.ticketmaster.com/discovery/v2/events.json?page={index}&dmaId=366&size=200&apikey={api_key}'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    data=response.text\n",
    "\n",
    "    parsed = json.loads(data)\n",
    "\n",
    "    # Combine all the events\n",
    "    try:\n",
    "        list_of_events.extend(parsed.get('_embedded').get('events'))\n",
    "        index+=1\n",
    "    except:\n",
    "        print(\"successfully pulled\",index-1,\"pages and\",len(list_of_events), \"events\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-forth",
   "metadata": {},
   "source": [
    "# GET EVENT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "peripheral-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the events in a dataframe\n",
    "events = pd.DataFrame({'events':list_of_events})\n",
    "\n",
    "# Break the disctionaries into columns\n",
    "# Now we have 1 column for each sales, locale, dates, etc\n",
    "iteration_1 = events['events'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "improving-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the event names and add them to a dataframe\n",
    "list_of_event_names = [event.get('name') for event in list_of_events]\n",
    "list_of_event_ids = [event.get('id') for event in list_of_events]\n",
    "test = [event.get('test') for event in list_of_events]\n",
    "\n",
    "event_names_df = pd.DataFrame({'event name':list_of_event_names, 'event id':list_of_event_ids, 'is this entry a test?':test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-leadership",
   "metadata": {},
   "source": [
    "## VENUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prescribed-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_df = iteration_1._embedded.apply(pd.Series)\n",
    "\n",
    "list_of_venue_dictionaries = [embedded_df.venues[index][0] for index in range(len(embedded_df.venues))] \n",
    "\n",
    "venue_df = pd.DataFrame({'venue_dictionaries':list_of_venue_dictionaries})\n",
    "venue_final = pd.DataFrame(venue_df['venue_dictionaries'].apply(pd.Series)['name'])\n",
    "venue_final.rename({'name': 'venue name'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-adult",
   "metadata": {},
   "source": [
    "## SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "central-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the sales column into columns\n",
    "# We now have public sales and presales\n",
    "sales = iteration_1.sales.apply(pd.Series)\n",
    "\n",
    "# Break the public sales into columns\n",
    "sales_public = sales.public.apply(pd.Series)\n",
    "\n",
    "# rename the public sales columns to distinguish them from the rest\n",
    "for col in sales_public.columns:\n",
    "    sales_public.rename({col:f'public sales {col}'}, axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "\n",
    "# Create a dataframe to add the final product of the steps below\n",
    "presale_final = pd.DataFrame()\n",
    "\n",
    "# Looping through the presales column to extract the information from the dictionaries\n",
    "# each presales row will have a dictionary with all the presale methods \n",
    "for presales_list_element in sales.presales:\n",
    "    \n",
    "# \"\"\"\n",
    "# If there is no presale, the entire rowcell will be NaN\n",
    "# In this case the pandas.isnull method will give us an error\n",
    "# We will need to add a new blank row in the except statement\n",
    "# We will also add a new column called 'blank' to make sure we don't mess the other columns names\n",
    "# The column will be dropped at the end\n",
    "# \"\"\"\n",
    "    try:\n",
    "        length = len(pd.isnull(presales_list_element))\n",
    "    except:\n",
    "        dict_holder_df = pd.DataFrame({'blank':[np.nan]})\n",
    "        presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "        continue\n",
    "\n",
    "# \"\"\"\n",
    "# The dictionaries will contain the name of the presale as well as the end and start date. \n",
    "# We need to extract the name and move it to the column names so that we can distinguish the start and end dates of each presale\n",
    "# \"\"\"\n",
    "    dict_holder_df = pd.DataFrame()\n",
    "    for presale_dict in presales_list_element:\n",
    "        \n",
    "        start_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[0]\n",
    "        end_time_title = presale_dict['name'] + ' ' + list(presale_dict.keys())[1]\n",
    "        \n",
    "        holder_df = pd.DataFrame({start_time_title:[presale_dict['startDateTime']], end_time_title:[presale_dict['endDateTime']]})\n",
    "        dict_holder_df = pd.concat([dict_holder_df,holder_df], axis=1)\n",
    "        \n",
    "    presale_final = pd.concat([presale_final,dict_holder_df],axis=0)\n",
    "\n",
    "\n",
    "# Reset the index so that we can merge\n",
    "# drop the blank column\n",
    "presale_final.reset_index(inplace=True)\n",
    "presale_final.drop(columns=['index', 'blank'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-forest",
   "metadata": {},
   "source": [
    "# DATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "danish-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "# Break the dates column into columns\n",
    "# There are a lot of dates trapped into disctionaries. Use pd.Series multiple times to untangle everything\n",
    "dates = iteration_1.dates.apply(pd.Series)\n",
    "start_dates = dates.start.apply(pd.Series)\n",
    "status = dates.status.apply(pd.Series)\n",
    "initial_start_date = dates.initialStartDate.apply(pd.Series)\n",
    "\n",
    "# Make sure there are no weird columns created by pd.series\n",
    "initial_start_date = initial_start_date[['dateTime', 'localDate', 'localTime']]\n",
    "\n",
    "# rename the dates columns to distinguish them\n",
    "for col in start_dates.columns:\n",
    "    start_dates.rename({col:f'event start {col}'}, axis = 1, inplace = True)\n",
    "\n",
    "for col in initial_start_date.columns:\n",
    "    initial_start_date.rename({col:f'event initial start {col}'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-norwegian",
   "metadata": {},
   "source": [
    "# CLASSIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "formal-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',500)\n",
    "\n",
    "list_of_classification_dictionaries = [iteration_1.classifications[index][0] for index in range(len(iteration_1.classifications))] \n",
    "\n",
    "classification_df = pd.DataFrame({'classification_dictionaries':list_of_classification_dictionaries})\n",
    "classification_df_broken_in_columns = classification_df['classification_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "classification_df_primary_family = classification_df_broken_in_columns[['primary', 'family']]\n",
    "columns_of_interest = list(classification_df_broken_in_columns.columns)\n",
    "columns_of_interest.remove('primary')\n",
    "columns_of_interest.remove('family')\n",
    "\n",
    "classification_df_final = pd.DataFrame()\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    series_holder = classification_df_broken_in_columns[column].apply(pd.Series)\n",
    "    series_holder.drop(columns = ['id'], inplace=True)\n",
    "    series_holder.rename({series_holder.columns[0]:column}, inplace = True, axis = 1)\n",
    "    classification_df_final = pd.concat([classification_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-queen",
   "metadata": {},
   "source": [
    "# PROMOTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "geographic-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the promoters column into columns\n",
    "promoters = iteration_1.promoters.apply(pd.Series)\n",
    "\n",
    "for index in range(len(promoters.columns)):\n",
    "    promoters.rename({promoters.columns[index]: 'event promoter' + ' ' + str(promoters.columns[index]+1)}, axis = 1, inplace = True)\n",
    "\n",
    "promoters_df_final = pd.DataFrame()\n",
    "\n",
    "for column in promoters.columns:\n",
    "    series_holder = promoters[column].apply(pd.Series)\n",
    "    series_holder = series_holder[['name', 'description']]\n",
    "    series_holder.rename({'name':column + ' ' + 'name', 'description':column + ' ' + 'description'}, inplace = True, axis = 1)\n",
    "    promoters_df_final = pd.concat([promoters_df_final,series_holder], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-terminology",
   "metadata": {},
   "source": [
    "# PRICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "material-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of events with no price information\n",
    "# These would be useless in any kind of project\n",
    "price_holder = iteration_1.dropna(subset=['priceRanges'], axis = 0) \n",
    "\n",
    "list_of_pricerange_dictionaries = [price_holder.priceRanges[index][0] for index in list(price_holder.index)] \n",
    "\n",
    "pricerange_df = pd.DataFrame({'pricerange_dictionaries':list_of_pricerange_dictionaries})\n",
    "pricerange_df_broken_in_columns = pricerange_df['pricerange_dictionaries'].apply(pd.Series)\n",
    "\n",
    "\n",
    "pricerange_df_broken_in_columns = pricerange_df_broken_in_columns[['type', 'min', 'max']]\n",
    "pricerange_df_broken_in_columns.rename({'type':'price type', 'min':'price min', 'max':'price max'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-telling",
   "metadata": {},
   "source": [
    "# TICKET LIMIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "raising-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_1.columns\n",
    "ticket_limit = pd.DataFrame(iteration_1['ticketLimit'].apply(pd.Series))\n",
    "ticket_limit = pd.DataFrame(ticket_limit['info']) \n",
    "ticket_limit.rename({'info':'ticket limit'},axis = 1, inplace = True)\n",
    "please_note = pd.DataFrame(iteration_1['pleaseNote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-framing",
   "metadata": {},
   "source": [
    "# MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deluxe-fossil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pull resulted in 0 new rows\n",
      "This pull resulted in 0 new events\n"
     ]
    }
   ],
   "source": [
    "new_dataset = pd.concat([\n",
    "    event_names_df\n",
    "    ,venue_final\n",
    "    ,iteration_1['info']\n",
    "    ,sales_public\n",
    "    ,presale_final\n",
    "    ,start_dates\n",
    "    ,status\n",
    "    ,initial_start_date\n",
    "    ,classification_df_final\n",
    "    ,promoters_df_final\n",
    "    ,pricerange_df_broken_in_columns\n",
    "    ,ticket_limit\n",
    "    ,please_note\n",
    "    \n",
    "], axis = 1)\n",
    "\n",
    "list_of_groupby_columns = list(new_dataset.columns)\n",
    "\n",
    "datatypes_df = pd.DataFrame(new_dataset.dtypes, columns = ['data type']).reset_index()\n",
    "\n",
    "list_of_string_variables = list(datatypes_df[datatypes_df['data type'] == 'object']['index'])\n",
    "list_of_boolean_variables = list(datatypes_df[datatypes_df['data type'] == 'bool']['index'])\n",
    "list_of_numeric_variables = list(datatypes_df[datatypes_df['data type'] == 'float']['index'])\n",
    "\n",
    "for var in list_of_groupby_columns:\n",
    "    if var in list_of_string_variables:\n",
    "        new_dataset.fillna({var: '0'}, inplace = True)\n",
    "    else:\n",
    "        new_dataset.fillna({var: 0}, inplace = True)\n",
    "        \n",
    "today_timestamp = pd.to_datetime(\"today\")\n",
    "today_date = today.date()\n",
    "new_dataset['timestamp of data pull'] = today_timestamp\n",
    "new_dataset['date of data pull'] = today_date\n",
    "\n",
    "try:\n",
    "    current_dataset = pd.read_csv('final_ticketmaster_dataset.csv')\n",
    "    appended_dataset = pd.concat([current_dataset, new_dataset], axis = 0)\n",
    "    appended_dataset.drop(columns = [appended_dataset.columns[0]], inplace = True)\n",
    "\n",
    "    appended_dataset['timestamp of data pull'] = pd.to_datetime(appended_dataset['timestamp of data pull'])\n",
    "    appended_dataset['date of data pull'] = pd.to_datetime(appended_dataset['date of data pull'])\n",
    "\n",
    "    updated_dataset = appended_dataset.groupby(list_of_groupby_columns, as_index = False) \\\n",
    "    .agg({'date of data pull':'min', 'timestamp of data pull':'min'}) \n",
    "\n",
    "    updated_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "\n",
    "    extra_events = updated_dataset['event id'].nunique() - current_dataset['event id'].nunique()  \n",
    "    extra_rows = updated_dataset.shape[0] - current_dataset.shape[0]  \n",
    "    print ('This pull resulted in', extra_rows, 'new rows')\n",
    "    print ('This pull resulted in', extra_events, 'new events')\n",
    "\n",
    "\n",
    "except:\n",
    "    new_dataset.to_csv('final_ticketmaster_dataset.csv')\n",
    "    print('except run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-madagascar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
